./tests/conftest.py:    cluster.slave_ips = ['10.0.0.2']
./tests/conftest.py:    cluster.slave_hosts = ['slave1.hostname']
./tests/conftest.py:    cluster.slave_private_hosts = ['slave1.privatehostname']
./tests/conftest.py:        '--num-slaves', '1',
./tests/conftest.py:        '--num-slaves', '1',
./flintrock/flintrock.py:@click.option('--num-slaves', type=click.IntRange(min=1), required=True)
./flintrock/flintrock.py:        num_slaves,
./flintrock/flintrock.py:            num_slaves=num_slaves,
./flintrock/flintrock.py:@cli.command(name='add-slaves')
./flintrock/flintrock.py:@click.option('--num-slaves', type=click.IntRange(min=1), required=True)
./flintrock/flintrock.py:def add_slaves(
./flintrock/flintrock.py:        num_slaves,
./flintrock/flintrock.py:    Add slaves to an existing cluster.
./flintrock/flintrock.py:    Flintrock will configure new slaves based on information queried
./flintrock/flintrock.py:            "Cannot add slaves to cluster '{c}' since it does not "
./flintrock/flintrock.py:    cluster.add_slaves_check()
./flintrock/flintrock.py:        cluster.add_slaves(
./flintrock/flintrock.py:            num_slaves=num_slaves,
./flintrock/flintrock.py:@cli.command(name='remove-slaves')
./flintrock/flintrock.py:@click.option('--num-slaves', type=click.IntRange(min=1), required=True)
./flintrock/flintrock.py:def remove_slaves(
./flintrock/flintrock.py:        num_slaves,
./flintrock/flintrock.py:    Remove slaves from an existing cluster.
./flintrock/flintrock.py:    if num_slaves > cluster.num_slaves:
./flintrock/flintrock.py:            "Warning: Cluster has {c} slave{cs}. "
./flintrock/flintrock.py:            "You asked to remove {n} slave{ns}."
./flintrock/flintrock.py:                c=cluster.num_slaves,
./flintrock/flintrock.py:                cs='' if cluster.num_slaves == 1 else 's',
./flintrock/flintrock.py:                n=num_slaves,
./flintrock/flintrock.py:                ns='' if num_slaves == 1 else 's'))
./flintrock/flintrock.py:        num_slaves = cluster.num_slaves
./flintrock/flintrock.py:            text=("Are you sure you want to remove {n} slave{s} from this cluster?"
./flintrock/flintrock.py:                      n=num_slaves,
./flintrock/flintrock.py:                      s='' if num_slaves == 1 else 's')),
./flintrock/flintrock.py:    logger.info("Removing {n} slave{s}..."
./flintrock/flintrock.py:                    n=num_slaves,
./flintrock/flintrock.py:                    s='' if num_slaves == 1 else 's'))
./flintrock/flintrock.py:    cluster.remove_slaves(
./flintrock/flintrock.py:        num_slaves=num_slaves)
./flintrock/flintrock.py:        num_nodes = len(cluster.slave_ips) + 1  # TODO: cluster.num_nodes
./flintrock/flintrock.py:        'add-slaves': ec2_configs,
./flintrock/flintrock.py:        'remove-slaves': ec2_configs,
./flintrock/services.py:        This method is role-agnostic; it runs on both the cluster master and slaves.
./flintrock/services.py:        This method is role-agnostic; it runs on both the cluster master and slaves.
./flintrock/services.py:        slaves.
./flintrock/services.py:    def configure_slave(
./flintrock/services.py:        Configure a service slave on a node via the provided SSH client after the
./flintrock/services.py:        This method is meant to be called once on each cluster slave.
./flintrock/services.py:            'hadoop/conf/slaves',
./flintrock/services.py:    # TODO: Convert this into start_master() and split master- or slave-specific
./flintrock/services.py:    #       stuff out of configure() into configure_master() and configure_slave().
./flintrock/services.py:            'spark/conf/slaves',
./flintrock/services.py:    # TODO: Convert this into start_master() and split master- or slave-specific
./flintrock/services.py:    #       stuff out of configure() into configure_master() and configure_slave().
./flintrock/services.py:    #       start_slave() can block until slave is fully up; that way we don't need
./flintrock/core.py:    def slave_ips(self) -> 'List[str]':
./flintrock/core.py:        A list of the IP addresses of the slaves.
./flintrock/core.py:    def slave_hosts(self) -> 'List[str]':
./flintrock/core.py:        A list of the hostnames of the slaves.
./flintrock/core.py:    def num_slaves(self) -> int:
./flintrock/core.py:        How many slaves the cluster has.
./flintrock/core.py:        This is typically just len(self.slave_ips), but we need a separate
./flintrock/core.py:        property because slave IPs are not available when the cluster is
./flintrock/core.py:        many slaves there are.
./flintrock/core.py:        hosts = [self.master_ip] + self.slave_ips
./flintrock/core.py:    def add_slaves_check(self):
./flintrock/core.py:    def add_slaves(self, *, user: str, identity_file: str, new_hosts: list):
./flintrock/core.py:        Add new slaves to the cluster.
./flintrock/core.py:            add_slaves(self, *, user: str, identity_file: str, num_slaves: int, **provider_specific_options)
./flintrock/core.py:        hosts = [self.master_ip] + self.slave_ips
./flintrock/core.py:            add_slaves_node,
./flintrock/core.py:    def remove_slaves(self, *, user: str, identity_file: str):
./flintrock/core.py:        Remove some slaves from the cluster.
./flintrock/core.py:            remove_slaves(self, *, user: str, identity_file: str, num_slaves: int)
./flintrock/core.py:        This method should be called after the provider has removed the slaves
./flintrock/core.py:        the relevant slaves are no longer part of the cluster.
./flintrock/core.py:            remove_slaves_node,
./flintrock/core.py:        hosts = [self.master_ip] + self.slave_ips
./flintrock/core.py:            target_hosts = [self.master_ip] + self.slave_ips
./flintrock/core.py:            target_hosts = [self.master_ip] + self.slave_ips
./flintrock/core.py:        'slave_ips': '\n'.join(cluster.slave_ips),
./flintrock/core.py:        'slave_hosts': '\n'.join(cluster.slave_hosts),
./flintrock/core.py:        'slave_private_hosts': '\n'.join(cluster.slave_private_hosts),
./flintrock/core.py:    Cluster methods like provision_node() and add_slaves_node() should
./flintrock/core.py:    hosts = [cluster.master_ip] + cluster.slave_ips
./flintrock/core.py:    This method is role-agnostic; it runs on both the cluster master and slaves.
./flintrock/core.py:    This method is role-agnostic; it runs on both the cluster master and slaves.
./flintrock/core.py:def add_slaves_node(
./flintrock/core.py:    This method is role-agnostic; it runs on both the cluster master and slaves.
./flintrock/core.py:def remove_slaves_node(
./flintrock/core.py:    Update the services on a node to remove the provided slaves.
./flintrock/core.py:    This method is role-agnostic; it runs on both the cluster master and slaves.
./flintrock/core.py:    This method is role-agnostic; it runs on both the cluster master and slaves.
./flintrock/core.py:    This method is role-agnostic; it runs on both the cluster master and slaves.
./flintrock/config.yaml.template:  num-slaves: 1
./flintrock/ec2.py:            slave_instances: "List[boto3.resources.factory.ec2.Instance]",
./flintrock/ec2.py:        self.slave_instances = slave_instances
./flintrock/ec2.py:            return [self.master_instance] + self.slave_instances
./flintrock/ec2.py:            return self.slave_instances
./flintrock/ec2.py:    def slave_ips(self):
./flintrock/ec2.py:        return [i.public_ip_address for i in self.slave_instances]
./flintrock/ec2.py:    def slave_hosts(self):
./flintrock/ec2.py:        return [i.public_dns_name for i in self.slave_instances]
./flintrock/ec2.py:    def slave_private_hosts(self):
./flintrock/ec2.py:        return [i.private_dns_name for i in self.slave_instances]
./flintrock/ec2.py:    def num_slaves(self):
./flintrock/ec2.py:        return len(self.slave_instances)
./flintrock/ec2.py:        master and slave IP addresses and hostnames.
./flintrock/ec2.py:            (self.master_instance, self.slave_instances) = _get_cluster_master_slaves(instances)
./flintrock/ec2.py:    def add_slaves_check(self):
./flintrock/ec2.py:                attempted_command='add-slaves',
./flintrock/ec2.py:    def add_slaves(
./flintrock/ec2.py:            num_slaves: int,
./flintrock/ec2.py:        self.add_slaves_check()
./flintrock/ec2.py:            new_slave_instances = _create_instances(
./flintrock/ec2.py:                num_instances=num_slaves,
./flintrock/ec2.py:            slave_tags = [
./flintrock/ec2.py:                {'Key': 'flintrock-role', 'Value': 'slave'},
./flintrock/ec2.py:                {'Key': 'Name', 'Value': '{c}-slave'.format(c=self.name)}]
./flintrock/ec2.py:            slave_tags += tags
./flintrock/ec2.py:                        {'Name': 'instance-id', 'Values': [i.id for i in new_slave_instances]}
./flintrock/ec2.py:                .create_tags(Tags=slave_tags))
./flintrock/ec2.py:            existing_slaves = {i.public_ip_address for i in self.slave_instances}
./flintrock/ec2.py:            self.slave_instances += new_slave_instances
./flintrock/ec2.py:            new_slaves = {i.public_ip_address for i in self.slave_instances} - existing_slaves
./flintrock/ec2.py:            super().add_slaves(
./flintrock/ec2.py:                new_hosts=new_slaves)
./flintrock/ec2.py:                cleanup_instances = new_slave_instances
./flintrock/ec2.py:    def remove_slaves(self, *, user: str, identity_file: str, num_slaves: int):
./flintrock/ec2.py:        # self.remove_slaves_check() (?)
./flintrock/ec2.py:            self.slave_instances,
./flintrock/ec2.py:        removed_slave_instances, self.slave_instances = \
./flintrock/ec2.py:            _instances[0:num_slaves], _instances[num_slaves:]
./flintrock/ec2.py:            super().remove_slaves(user=user, identity_file=identity_file)
./flintrock/ec2.py:        for instance in removed_slave_instances:
./flintrock/ec2.py:                    {'Name': 'instance-id', 'Values': [i.id for i in removed_slave_instances]}
./flintrock/ec2.py:                    ['  slaves:'] + (self.slave_hosts if self.num_slaves > 0 else [])))
./flintrock/ec2.py:        num_slaves,
./flintrock/ec2.py:    num_instances = num_slaves + 1
./flintrock/ec2.py:        slave_instances = cluster_instances[1:]
./flintrock/ec2.py:        slave_tags = [
./flintrock/ec2.py:            {'Key': 'flintrock-role', 'Value': 'slave'},
./flintrock/ec2.py:            {'Key': 'Name', 'Value': '{c}-slave'.format(c=cluster_name)}]
./flintrock/ec2.py:        slave_tags += tags
./flintrock/ec2.py:                    {'Name': 'instance-id', 'Values': [i.id for i in slave_instances]}
./flintrock/ec2.py:            .create_tags(Tags=slave_tags))
./flintrock/ec2.py:            slave_instances=slave_instances)
./flintrock/ec2.py:def _get_cluster_master_slaves(
./flintrock/ec2.py:    Get the master and slave instances from a set of raw EC2 instances representing
./flintrock/ec2.py:    slave_instances = []
./flintrock/ec2.py:                elif tag['Value'] == 'slave':
./flintrock/ec2.py:                    slave_instances.append(instance)
./flintrock/ec2.py:    # elif not slave_instances:
./flintrock/ec2.py:    #     print("Warning: No slaves found.", file=sys.stderr)
./flintrock/ec2.py:    return (master_instance, slave_instances)
./flintrock/ec2.py:    (master_instance, slave_instances) = _get_cluster_master_slaves(instances)
./flintrock/ec2.py:        slave_instances=slave_instances)
./flintrock/templates/hadoop/conf/nodes:{slave_private_hosts}
./flintrock/templates/spark/conf/nodes:{slave_private_hosts}
./README.md:    --num-slaves 1 \
./README.md:flintrock add-slaves test-cluster --num-slaves 2
./README.md:flintrock remove-slaves test-cluster --num-slaves 1
./README.md:flintrock launch test-cluster --num-slaves 10
./README.md:  num-slaves: 1
./README.md:    --num-slaves 10 \
./README.md:| 1 slave       | 2m 06s                |     8m 44s              |
./README.md:| 50 slaves     | 2m 30s                |    37m 30s              |
./README.md:| 100 slaves    | 2m 42s                | 1h 06m 05s              |
./README.md:* **Slow launches**: spark-ec2 cluster launch times increase linearly with the number of slaves being created. For example, it takes spark-ec2 **[over an hour](https://issues.apache.org/jira/browse/SPARK-5189)** to launch a cluster with 100 slaves. ([SPARK-4325](https://issues.apache.org/jira/browse/SPARK-4325), [SPARK-5189](https://issues.apache.org/jira/browse/SPARK-5189))
./README.md:* **Un-resizable clusters**: Adding or removing slaves from an existing spark-ec2 cluster is not possible. ([SPARK-2008](https://issues.apache.org/jira/browse/SPARK-2008))
./CHANGES.md:  `add-slaves` and `remove-slaves` commands.
./CHANGES.md:* [#115]: You can no longer launch clusters with 0 slaves. The
